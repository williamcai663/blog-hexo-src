---
title: spark访问mysql数据库实例
date: 2022-08-18 19:41:07
tags: 大数据,spark
---

1、上传msyql的jar到spark的目录下
` ~/.m2/repository/mysql/mysql-connector-java/5.1.47  scp mysql-connector-java-5.1.47.jar root@10.90.9.114:/opt/spark-2.2.3-bin-hadoop2.6/jars`

2、启动spark 客户端
```
[root@localhost ~]# !1023
spark-shell --master local[2]
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/08/18 07:36:46 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.90.9.114 instead (on interface eth0)
22/08/18 07:36:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/08/18 07:36:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://10.90.9.114:4040
Spark context available as 'sc' (master = local[2], app id = local-1660822607218).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.3
      /_/
```
3、编写访问代码
`spark.read
.format("jdbc")
.option("driver", "com.mysql.jdbc.Driver")            
.option("url", "jdbc:mysql://10.90.9.120:3306/sys?useSSL=false")   
.option("dbtable", "sys_config")                   
.option("user", "root").option("password","xxxx").load().show(10)`

4、执行代码
```
scala> spark.read
res12: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59fa5a39

scala> .format("jdbc")
res13: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59fa5a39

scala> .option("driver", "com.mysql.jdbc.Driver")
res14: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59fa5a39

scala> .option("url", "jdbc:mysql://10.90.9.120:3306/sys?useSSL=false")
res15: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59fa5a39

scala> .option("dbtable", "sys_config")
res16: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@59fa5a39

scala> .option("user", "root").option("password","Xx00123").load().show(10)
+--------------------+-----+-------------------+------+
|            variable|value|           set_time|set_by|
+--------------------+-----+-------------------+------+
|diagnostics.allow...|  OFF|2021-08-04 11:39:10|  null|
|diagnostics.inclu...|  OFF|2021-08-04 11:39:10|  null|
|ps_thread_trx_inf...|65535|2021-08-04 11:39:10|  null|
|statement_perform...|  100|2021-08-04 11:39:10|  null|
|statement_perform...| null|2021-08-04 11:39:10|  null|
|statement_truncat...|   64|2021-08-04 11:39:10|  null|
+--------------------+-----+-------------------+------+


scala>
```